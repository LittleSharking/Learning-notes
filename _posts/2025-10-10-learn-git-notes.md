
# 软件工程原理与实践实验报告

<center>姓名：李东旭  学号：23020007056</center>

| 姓名和学号         | 李东旭，23020007056                  |
| -------------------- | ------------------------------ |
| 课程 | 中国海洋大学25秋《软件工程原理与实践》 |
| 实验名称           | 实验2：深度学习基础          |

## 一、实验内容

### 1. 代码练习
#### 1.1 pytorch 基础练习
##### 1.1.1 定义数据
一些基本的张量的定义方式

<img width="474" height="456" alt="6ba1f06eec3fc25606712e55f2da74e7" src="https://github.com/user-attachments/assets/337681c2-5513-41a7-ab76-907572afd2ce" />
<img width="475" height="456" alt="8bc0c5a8c3c9064099bfcb8a4018263d" src="https://github.com/user-attachments/assets/1c0ff82b-8185-4091-8f96-f87a75e2eb16" />

##### 1.1.2 定义操作
（1）一些基本的对张量的操作

<img width="578" height="580" alt="48dc6bd807cf586d66fe056258c147c5" src="https://github.com/user-attachments/assets/ab9baa57-2fba-4b2b-b3c7-ce9ac56b2da4" />

（2）使用matplotlib进行绘图

<img width="498" height="580" alt="16a691942afbf3c7fb3644b9d678f385" src="https://github.com/user-attachments/assets/e3af906d-c250-4692-a71b-bb9d219fc27e" />

<img width="492" height="580" alt="04eed12a3909461f935cd4a60a9363f2" src="https://github.com/user-attachments/assets/26bb0de0-d0a4-40fd-9916-041c48ad8933" />

#### 1.2 螺旋数据分类
##### 1.2.1 构建线性模型分类

<img width="657" height="550" alt="879966739680a50e64423e68a81bb9f0" src="https://github.com/user-attachments/assets/43a8c849-b725-438a-9b3d-98f1b4420111" />

<img width="654" height="550" alt="a4976186528acff7cec5437338ae5a00" src="https://github.com/user-attachments/assets/5b782134-b3cb-4e8d-896e-21674dc9477e" />

<img width="500" height="550" alt="9c2c09ee7c16949c35c72f617f3f25fc" src="https://github.com/user-attachments/assets/99f6e20d-1059-447b-a531-77545195216c" />

##### 1.2.2 构建两层神经网络分类
添加ReLU激活函数之后看看效果

<img width="450" height="550" alt="ae49bb87aa78055f8f087a2951ed00e7" src="https://github.com/user-attachments/assets/7d0d86b0-e57e-4a83-8890-4236c27fd3fd" />

如图所示，我得到的图相比之前的是有一些变化（分界线开始弯曲了），但还是没有示例里给的那个变化那么大。
### 2. 问题总结
#### 2.1 AlexNet有哪些特点？为什么可以比LeNet取得更好的性能？
AlexNet的主要特点是其更深的结构、采用ReLU激活函数缓解梯度消失、使用Dropout层减少过拟合、以及引入数据增强和局部响应归一化来提升泛化能力。它能取得更好性能的核心原因是，更深的网络结构能够学习更复杂的特征表示，而ReLU和Dropout等关键技术有效改善了训练过程的效率和稳定性，再加上GPU计算带来的大规模训练能力，共同使其在复杂数据集上表现出远超LeNet的识别精度。
#### 2.2 激活函数有哪些作用？​​
激活函数的核心作用是为神经网络引入非线性变换，使网络能够学习和模拟复杂的现实模式。它通过决定神经元是否被激活以及激活程度，来控制网络中信息的流动。此外，不同的激活函数直接影响梯度在反向传播中的行为，是缓解梯度消失或爆炸问题、确保模型能够有效训练的关键。
#### 2.3 梯度消失现象是什么？​​
梯度消失是指在深层神经网络的训练过程中，误差反向传播时，梯度值随着层数的增加而指数级减小，导致网络靠近输入层的参数更新变得极其缓慢甚至停滞，从而使得网络难以训练。
#### 2.4 神经网络是更宽好还是更深好？​​
一般来说，更深的网络通常优于更宽的网络。深度网络能通过层次化结构学习从低级到高级的抽象特征，参数效率更高。而更宽的网络虽然能增加模型容量，但更容易过拟合且计算成本高昂。
#### ​​2.5 为什么要使用Softmax？​​
Softmax函数主要用于多分类任务的输出层，其作用是将神经网络输出的多个原始得分转换为一个总和为1的概率分布。这样，每个类别的输出值可以直观地解释为属于该类别的概率，这不仅使得模型的预测结果易于理解，更重要的是，它与交叉熵损失函数配合使用，能够有效地衡量预测概率与真实标签之间的差异，从而指导模型优化。
#### ​​2.6 SGD 和 Adam 哪个更有效？​​
Adam优化器因其自适应学习率特性，通常能实现更快的收敛速度且对超参数不敏感，在实践中更受欢迎，尤其适用于大多数深度学习应用。而SGD在精心调参后可能找到泛化能力更好的模型最优解，但需要更多的调优经验。因此，Adam通常是默认的实用选择，而在追求极致模型性能时可以考虑SGD。

## 二、问题总结与体会
通过本次实验的代码练习和理论思考，我对深度学习的基础概念有了更直观和深入的理解。
在代码练习部分，我练习了pytorch基本的操作并且实现了基于PyTorch的螺旋数据分类。最初使用线性模型时，效果很不理想，之后，在模型中加入了ReLU激活函数构建两层神经网络后，决策边界开始出现弯曲，但是仅按照给的示例来操作效果是有但是很不理想，我在ai的帮助下又添加了一些内容才得到了一张弯曲程度（或者说是拟合程度？）和效果图差不多的结果。
总而言之，本次实验让我不仅学习了人工智能的一些基本的知识，更通过代码运行和结果对比，明白了ReLU函数等其他手段的作用和重要性。
